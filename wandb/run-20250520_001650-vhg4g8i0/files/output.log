Starting training run with config: {'batch_size': 64, 'cell_type': 'lstm', 'clip': 1, 'data_dir': '.', 'dropout': 0, 'embedding_size': 128, 'hidden_size': 256, 'language': 'ta', 'learning_rate': 0.001, 'max_seq_len': 50, 'n_epochs': 15, 'num_layers': 5, 'teacher_forcing_ratio': 0.5, 'use_attention': True}
Attempting to load data from: .\ta\lexicons\ta.translit.sampled.train.tsv
Source vocabulary size: 30
Target vocabulary size: 50
Sample Source Vocab: [('<PAD>', 0), ('<UNK>', 1), ('<SOS>', 2), ('<EOS>', 3), ('a', 4), ('b', 5), ('c', 6), ('d', 7), ('e', 8), ('f', 9)]
Sample Target Vocab: [('<PAD>', 0), ('<UNK>', 1), ('<SOS>', 2), ('<EOS>', 3), ('ஃ', 4), ('அ', 5), ('ஆ', 6), ('இ', 7), ('ஈ', 8), ('உ', 9)]
Successfully loaded Dakshina dataset for ta
Train set: 68218 examples
Dev set: 6827 examples
Test set: 6864 examples
Max sequence length: 50
Using device: cuda
The model has 5,437,234 trainable parameters
Training:  94%|██████████████████████████████████▋  | 999/1066 [09:51<00:33,  1.99it/s]
