Looking for training file at: ta\lexicons\ta.translit.sampled.train.tsv
Special tokens in vocabulary:
Source vocab keys: ['<PAD>', '<UNK>', '<SOS>', '<EOS>', 'ஃ', 'அ', 'ஆ', 'இ', 'ஈ', 'உ']
Target vocab keys: ['<PAD>', '<UNK>', '<SOS>', '<EOS>', 'a', 'b', 'c', 'd', 'e', 'f']
Successfully loaded Dakshina dataset for ta
Train set: 68218 examples
Dev set: 6827 examples
Test set: 6864 examples
Source vocabulary size: 50
Target vocabulary size: 30
The model has 207,646 trainable parameters
100%|██████████████████████████████████████████████████████████████████████| 1066/1066 [02:39<00:00,  6.67it/s]
Epoch: 01 | Time: 2m 44s
	Train Loss: 2.493
	 Val. Loss: 2.556
	 Val. Accuracy: 0.000
100%|██████████████████████████████████████████████████| 1066/1066 [02:36<00:00,  6.79it/s]
Epoch: 02 | Time: 2m 41s
	Train Loss: 2.279
	 Val. Loss: 2.518
	 Val. Accuracy: 0.000
100%|██████████████████████████████████████████████████| 1066/1066 [02:27<00:00,  7.22it/s]
Epoch: 03 | Time: 2m 31s
	Train Loss: 2.181
	 Val. Loss: 2.443
	 Val. Accuracy: 0.000
100%|█████████████████████████████████████████████████████████████████| 1066/1066 [02:28<00:00,  7.20it/s]
Epoch: 04 | Time: 2m 32s
	Train Loss: 2.057
	 Val. Loss: 2.339
	 Val. Accuracy: 0.000
100%|█████████████████████████████████████████████████████████████████| 1066/1066 [02:20<00:00,  7.58it/s]
Epoch: 05 | Time: 2m 24s
	Train Loss: 1.944
	 Val. Loss: 2.278
	 Val. Accuracy: 0.000
100%|█████████████████████████████████████████████████████████████████| 1066/1066 [02:17<00:00,  7.76it/s]
Epoch: 06 | Time: 2m 21s
	Train Loss: 1.846
	 Val. Loss: 2.156
	 Val. Accuracy: 0.001
100%|█████████████████████████████████████████████████████████████████| 1066/1066 [02:26<00:00,  7.30it/s]
Epoch: 07 | Time: 2m 29s
	Train Loss: 1.770
	 Val. Loss: 2.125
	 Val. Accuracy: 0.002
100%|██████████████████████████████████████████| 1066/1066 [02:23<00:00,  7.41it/s]
Epoch: 08 | Time: 2m 27s
	Train Loss: 1.710
	 Val. Loss: 2.088
	 Val. Accuracy: 0.003
100%|█████████████████████████████| 1066/1066 [02:11<00:00,  8.08it/s]
Epoch: 09 | Time: 2m 15s
	Train Loss: 1.653
	 Val. Loss: 2.023
	 Val. Accuracy: 0.004
100%|█████████████████████████████| 1066/1066 [02:03<00:00,  8.63it/s]
Epoch: 10 | Time: 2m 6s
	Train Loss: 1.607
	 Val. Loss: 2.013
	 Val. Accuracy: 0.005
100%|█████████████████████████████| 1066/1066 [01:49<00:00,  9.70it/s]
Epoch: 11 | Time: 1m 52s
	Train Loss: 1.564
	 Val. Loss: 2.016
	 Val. Accuracy: 0.007
100%|█████████████████████████████| 1066/1066 [02:17<00:00,  7.76it/s]
Epoch: 12 | Time: 2m 20s
	Train Loss: 1.524
	 Val. Loss: 1.986
	 Val. Accuracy: 0.009
100%|█████████████████████████████| 1066/1066 [01:49<00:00,  9.74it/s]
Epoch: 13 | Time: 1m 52s
	Train Loss: 1.491
	 Val. Loss: 1.932
	 Val. Accuracy: 0.013
100%|█████████████████████████████| 1066/1066 [02:09<00:00,  8.25it/s]
Epoch: 14 | Time: 2m 14s
	Train Loss: 1.446
	 Val. Loss: 1.923
	 Val. Accuracy: 0.020
100%|█████████████████████████████| 1066/1066 [03:13<00:00,  5.52it/s]
Epoch: 15 | Time: 3m 19s
	Train Loss: 1.411
	 Val. Loss: 1.879
	 Val. Accuracy: 0.022
100%|███████████████████████| 1066/1066 [03:23<00:00,  5.24it/s]
Epoch: 16 | Time: 3m 28s
	Train Loss: 1.388
	 Val. Loss: 1.829
	 Val. Accuracy: 0.025
100%|███████████████████████| 1066/1066 [03:17<00:00,  5.39it/s]
Epoch: 17 | Time: 3m 23s
	Train Loss: 1.351
	 Val. Loss: 1.788
	 Val. Accuracy: 0.032
100%|███████████████████████| 1066/1066 [03:10<00:00,  5.61it/s]
Epoch: 18 | Time: 3m 13s
	Train Loss: 1.318
	 Val. Loss: 1.773
	 Val. Accuracy: 0.039
100%|███████████████████████| 1066/1066 [01:56<00:00,  9.13it/s]
Epoch: 19 | Time: 2m 0s
	Train Loss: 1.290
	 Val. Loss: 1.762
	 Val. Accuracy: 0.044
100%|██████████████████████████████████████████████| 1066/1066 [01:55<00:00,  9.22it/s]
Epoch: 20 | Time: 1m 59s
	Train Loss: 1.256
	 Val. Loss: 1.748
	 Val. Accuracy: 0.045
Test Loss: 1.774 | Test Accuracy: 0.043
Traceback (most recent call last):
  File "C:\Users\DELL\Documents\SEMESTER 8\Deep Learning\Assignment_3\DA6401-Assignment-3\src\training\train.py", line 205, in train_model
    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)
  File "C:\Users\DELL\anaconda3\envs\Transliteration\lib\json\__init__.py", line 180, in dump
    fp.write(chunk)
  File "C:\Users\DELL\anaconda3\envs\Transliteration\lib\encodings\cp1252.py", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode characters in position 1-7: character maps to <undefined>
