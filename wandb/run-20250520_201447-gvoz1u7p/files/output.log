Actual wandb run name: northern-sweep-3
Starting training run with config: {'batch_size': 32, 'cell_type': 'rnn', 'clip': 1, 'data_dir': '.', 'decoder_layers': 2, 'dropout': 0.2, 'embedding_size': 32, 'encoder_layers': 2, 'hidden_size': 128, 'language': 'ta', 'learning_rate': 0.00026257224945568227, 'max_seq_len': 50, 'momentum': 0.9, 'n_epochs': 10, 'optimizer': 'adam', 'rmsprop_alpha': 0.99, 'scheduler': 'plateau', 'scheduler_factor': 0.5, 'scheduler_patience': 5, 'scheduler_t_max': 20, 'teacher_forcing_ratio': 0.7, 'use_attention': False, 'weight_decay': 0.0001}
Looking for training file at: .\ta\lexicons\ta.translit.sampled.train.tsv
Special tokens in vocabulary:
Source vocab keys: ['<PAD>', '<UNK>', '<SOS>', '<EOS>', 'a', 'b', 'c', 'd', 'e', 'f']
Target vocab keys: ['<PAD>', '<UNK>', '<SOS>', '<EOS>', 'ஃ', 'அ', 'ஆ', 'இ', 'ஈ', 'உ']
Successfully loaded Dakshina dataset for ta
Train set: 68218 examples
Dev set: 6827 examples
Test set: 6864 examples
Source vocabulary size: 30
Target vocabulary size: 50
Max sequence length: 50
Using device: cuda
The model has 116,530 trainable parameters
C:\Users\DELL\anaconda3\envs\Transliteration\lib\site-packages\torch\optim\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Training: 100%|█████████████| 2132/2132 [04:36<00:00,  7.71it/s]
Evaluating Loss: 100%|███████████████████████████████| 214/214 [00:07<00:00, 30.25it/s]
Calculating Accuracy: 100%|██████████████████████████| 214/214 [00:02<00:00, 81.36it/s]
Epoch: 01 | Time: 4m 46s
	Train Loss: 0.5622
	 Val. Loss: 0.5514
	 Val. Accuracy: 0.0000
	 Learning Rate: 0.000263
Saved new best model to best-model-gvoz1u7p.pt
Training: 100%|████████████████████████████████████| 2132/2132 [05:05<00:00,  6.97it/s]
Evaluating Loss: 100%|███████████████████████████████| 214/214 [00:09<00:00, 21.45it/s]
Calculating Accuracy: 100%|██████████████████████████| 214/214 [00:03<00:00, 54.59it/s]
Epoch: 02 | Time: 5m 19s
	Train Loss: 0.4967
	 Val. Loss: 0.5578
	 Val. Accuracy: 0.0000
	 Learning Rate: 0.000263
Training: 100%|████████████████████████████████████| 2132/2132 [06:09<00:00,  5.77it/s]
Evaluating Loss: 100%|███████████████████████████████| 214/214 [00:09<00:00, 21.69it/s]
Calculating Accuracy: 100%|██████████████████████████| 214/214 [00:03<00:00, 55.26it/s]
Epoch: 03 | Time: 6m 23s
	Train Loss: 0.4863
	 Val. Loss: 0.5637
	 Val. Accuracy: 0.0000
	 Learning Rate: 0.000263
Training: 100%|████████████████████████████████████| 2132/2132 [06:20<00:00,  5.61it/s]
Evaluating Loss: 100%|███████████████████████████████| 214/214 [00:10<00:00, 20.91it/s]
Calculating Accuracy: 100%|██████████████████████████| 214/214 [00:04<00:00, 53.06it/s]
Epoch: 04 | Time: 6m 34s
	Train Loss: 0.4773
	 Val. Loss: 0.5645
	 Val. Accuracy: 0.0000
	 Learning Rate: 0.000263
Training: 100%|████████████████████████████████████| 2132/2132 [11:57<00:00,  2.97it/s]
Evaluating Loss: 100%|███████████████████████████████| 214/214 [00:09<00:00, 21.69it/s]
Calculating Accuracy: 100%|██████████████████████████| 214/214 [00:03<00:00, 68.53it/s]
Epoch: 05 | Time: 12m 10s
	Train Loss: 0.4711
	 Val. Loss: 0.5735
	 Val. Accuracy: 0.0004
	 Learning Rate: 0.000263
Training: 100%|████████████████████████████████████| 2132/2132 [06:19<00:00,  5.62it/s]
Evaluating Loss: 100%|███████████████████████████████| 214/214 [00:10<00:00, 21.02it/s]
Calculating Accuracy: 100%|██████████████████████████| 214/214 [00:05<00:00, 38.84it/s]
Epoch: 06 | Time: 6m 35s
	Train Loss: 0.4649
	 Val. Loss: 0.5660
	 Val. Accuracy: 0.0000
	 Learning Rate: 0.000263
Training: 100%|████████████████████████████████████| 2132/2132 [06:20<00:00,  5.60it/s]
Evaluating Loss: 100%|███████████████████████████████| 214/214 [00:10<00:00, 20.72it/s]
Calculating Accuracy: 100%|██████████████████████████| 214/214 [00:04<00:00, 44.91it/s]
Epoch: 07 | Time: 6m 35s
	Train Loss: 0.4606
	 Val. Loss: 0.5665
	 Val. Accuracy: 0.0000
	 Learning Rate: 0.000131
Training: 100%|████████████████████████████████████| 2132/2132 [06:16<00:00,  5.66it/s]
Evaluating Loss: 100%|███████████████████████████████| 214/214 [00:09<00:00, 21.51it/s]
Calculating Accuracy: 100%|██████████████████████████| 214/214 [00:03<00:00, 69.76it/s]
Epoch: 08 | Time: 6m 29s
	Train Loss: 0.4583
	 Val. Loss: 0.5732
	 Val. Accuracy: 0.0004
	 Learning Rate: 0.000131
Training: 100%|████████████████████████████████████| 2132/2132 [06:21<00:00,  5.59it/s]
Evaluating Loss: 100%|███████████████████████████████| 214/214 [00:10<00:00, 21.29it/s]
Calculating Accuracy: 100%|██████████████████████████| 214/214 [00:05<00:00, 41.90it/s]
Epoch: 09 | Time: 6m 36s
	Train Loss: 0.4576
	 Val. Loss: 0.5642
	 Val. Accuracy: 0.0000
	 Learning Rate: 0.000131
Training: 100%|████████████████████████████████████| 2132/2132 [05:58<00:00,  5.94it/s]
Evaluating Loss: 100%|███████████████████████████████| 214/214 [00:10<00:00, 21.35it/s]
Calculating Accuracy: 100%|██████████████████████████| 214/214 [00:05<00:00, 40.39it/s]
Epoch: 10 | Time: 6m 13s
	Train Loss: 0.4558
	 Val. Loss: 0.5634
	 Val. Accuracy: 0.0000
	 Learning Rate: 0.000131
Training finished.
Evaluating best model on test set...
C:\Users\DELL\Documents\SEMESTER 8\Deep Learning\Assignment_3\DA6401-Assignment-3\src\training\train.py:341: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(model_save_path, map_location=device))
Evaluating Loss: 100%|███████████████████████████████| 215/215 [00:10<00:00, 20.48it/s]
Calculating Accuracy: 100%|██████████████████████████| 215/215 [00:04<00:00, 47.86it/s]
Test Loss: 0.5568
Test Accuracy: 0.0000
Saved predictions to predictions/vanilla/predictions-gvoz1u7p.json
