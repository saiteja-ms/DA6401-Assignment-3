Starting training run with config: {'batch_size': 64, 'cell_type': 'lstm', 'clip': 1, 'data_dir': '.', 'decoder_layers': 2, 'dropout': 0, 'embedding_size': 32, 'encoder_layers': 1, 'hidden_size': 64, 'language': 'ta', 'learning_rate': 0.001, 'max_seq_len': 50, 'n_epochs': 15, 'teacher_forcing_ratio': 0.5, 'use_attention': True}
Attempting to load data from: .\ta\lexicons\ta.translit.sampled.train.tsv
Source vocabulary size: 30
Target vocabulary size: 50
Sample Source Vocab: [('<PAD>', 0), ('<UNK>', 1), ('<SOS>', 2), ('<EOS>', 3), ('a', 4), ('b', 5), ('c', 6), ('d', 7), ('e', 8), ('f', 9)]
Sample Target Vocab: [('<PAD>', 0), ('<UNK>', 1), ('<SOS>', 2), ('<EOS>', 3), ('ஃ', 4), ('அ', 5), ('ஆ', 6), ('இ', 7), ('ஈ', 8), ('உ', 9)]
Successfully loaded Dakshina dataset for ta
Train set: 68218 examples
Dev set: 6827 examples
Test set: 6864 examples
Max sequence length: 50
Using device: cuda
Traceback (most recent call last):
  File "C:\Users\DELL\Documents\SEMESTER 8\Deep Learning\Assignment_3\DA6401-Assignment-3\src\training\train.py", line 183, in train_model
    model = AttentionSeq2Seq(encoder, decoder, device).to(device) # Move overall model to device
  File "C:\Users\DELL\Documents\SEMESTER 8\Deep Learning\Assignment_3\DA6401-Assignment-3\src\models\seq2seq.py", line 106, in __init__
    assert encoder.num_layers == decoder.num_layers, \
AssertionError: Number of layers in encoder and decoder must be equal
