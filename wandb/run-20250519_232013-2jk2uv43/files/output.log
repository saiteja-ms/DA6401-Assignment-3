Starting training run with config: {'language': 'ta', 'embedding_size': 64, 'hidden_size': 128, 'encoder_layers': 1, 'decoder_layers': 1, 'cell_type': 'lstm', 'dropout': 0.2, 'learning_rate': 0.001, 'batch_size': 64, 'n_epochs': 20, 'clip': 1.0, 'teacher_forcing_ratio': 0.5, 'use_attention': False, 'data_dir': '.', 'max_seq_len': 50}
Attempting to load data from: .\ta\lexicons\ta.translit.sampled.train.tsv
Source vocabulary size: 30
Target vocabulary size: 50
Sample Source Vocab: [('<PAD>', 0), ('<UNK>', 1), ('<SOS>', 2), ('<EOS>', 3), ('a', 4), ('b', 5), ('c', 6), ('d', 7), ('e', 8), ('f', 9)]
Sample Target Vocab: [('<PAD>', 0), ('<UNK>', 1), ('<SOS>', 2), ('<EOS>', 3), ('ஃ', 4), ('அ', 5), ('ஆ', 6), ('இ', 7), ('ஈ', 8), ('உ', 9)]
Successfully loaded Dakshina dataset for ta
Train set: 68218 examples
Dev set: 6827 examples
Test set: 6864 examples
Max sequence length: 50
Using device: cuda
The model has 210,226 trainable parameters
Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1066/1066 [02:04<00:00,  8.57it/s]
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 29.72it/s]
Calculating Accuracy: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 107/107 [00:02<00:00, 46.61it/s]
Epoch: 01 | Time: 2m 10s
	Train Loss: 2.8365
	 Val. Loss: 3.0378
	 Val. Accuracy: 0.0000
Saved new best model to best-model-2jk2uv43.pt
Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1066/1066 [02:02<00:00,  8.73it/s]
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 33.51it/s]
Calculating Accuracy: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 107/107 [00:02<00:00, 42.34it/s]
Epoch: 02 | Time: 2m 7s
	Train Loss: 2.5732
	 Val. Loss: 2.9201
	 Val. Accuracy: 0.0000
Saved new best model to best-model-2jk2uv43.pt
Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1066/1066 [02:00<00:00,  8.88it/s]
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 107/107 [00:03<00:00, 34.74it/s]
Calculating Accuracy: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 107/107 [00:02<00:00, 42.76it/s]
Epoch: 03 | Time: 2m 5s
	Train Loss: 2.4786
	 Val. Loss: 2.9060
	 Val. Accuracy: 0.0000
Saved new best model to best-model-2jk2uv43.pt
Training:  37%|████████████████████████████████████████████▋                                                                           | 397/1066 [00:46<01:18,  8.52it/s]
Traceback (most recent call last):
  File "C:\Users\DELL\Documents\SEMESTER 8\Deep Learning\Assignment_3\DA6401-Assignment-3\src\training\train.py", line 198, in train_model
    train_loss = train_epoch(
  File "C:\Users\DELL\Documents\SEMESTER 8\Deep Learning\Assignment_3\DA6401-Assignment-3\src\training\train.py", line 82, in train_epoch
    loss.backward() # Backpropagate the error
  File "C:\Users\DELL\anaconda3\envs\Transliteration\lib\site-packages\torch\_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "C:\Users\DELL\anaconda3\envs\Transliteration\lib\site-packages\torch\autograd\__init__.py", line 347, in backward
    _engine_run_backward(
  File "C:\Users\DELL\anaconda3\envs\Transliteration\lib\site-packages\torch\autograd\graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
