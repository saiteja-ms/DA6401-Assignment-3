Starting training run with config: {'batch_size': 64, 'cell_type': 'gru', 'clip': 1, 'data_dir': '.', 'decoder_layers': 2, 'dropout': 0.2, 'embedding_size': 128, 'encoder_layers': 2, 'hidden_size': 128, 'language': 'ta', 'learning_rate': 0.001, 'max_seq_len': 50, 'n_epochs': 15, 'teacher_forcing_ratio': 0.5, 'use_attention': True}
Attempting to load data from: .\ta\lexicons\ta.translit.sampled.train.tsv
Source vocabulary size: 30
Target vocabulary size: 50
Sample Source Vocab: [('<PAD>', 0), ('<UNK>', 1), ('<SOS>', 2), ('<EOS>', 3), ('a', 4), ('b', 5), ('c', 6), ('d', 7), ('e', 8), ('f', 9)]
Sample Target Vocab: [('<PAD>', 0), ('<UNK>', 1), ('<SOS>', 2), ('<EOS>', 3), ('ஃ', 4), ('அ', 5), ('ஆ', 6), ('இ', 7), ('ஈ', 8), ('உ', 9)]
Successfully loaded Dakshina dataset for ta
Train set: 68218 examples
Dev set: 6827 examples
Test set: 6864 examples
Max sequence length: 50
Using device: cuda
The model has 507,954 trainable parameters
Training:   0%|                                                                                                                                                          | 0/1066 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "C:\Users\DELL\Documents\SEMESTER 8\Deep Learning\Assignment_3\DA6401-Assignment-3\src\training\train.py", line 216, in train_model
    train_loss = train_epoch(
  File "C:\Users\DELL\Documents\SEMESTER 8\Deep Learning\Assignment_3\DA6401-Assignment-3\src\training\train.py", line 95, in train_epoch
    trg_seq = trg[:, 1:]       # Shape [batch_size, trg_len-1]
IndexError: too many indices for tensor of dimension 1
